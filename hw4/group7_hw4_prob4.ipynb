{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the most probable value of $\\mu$, we need to find its posterior distribution. Let's begin with Bayes' Theorem:\n",
    "\n",
    "$$P(\\mu|D,I) = \\frac{P(D|\\mu,I)P(\\mu|I)}{P(D|I)}$$\n",
    "\n",
    "In this case, $D$ contains our two measurements. Assuming a constant prior on $\\mu$ and taking advantage of the fact that the evidence is independent of $\\mu$, the most likely value of $\\mu$ depends only on the likelihood. \n",
    "\n",
    "Assuming that the measurement error is Gaussian and that the measurements are independent, the likelihood is\n",
    "\\begin{aligned}\n",
    "P(D|\\mu,I) &= \\prod_{i\\in D} (2\\pi\\sigma_i^2)^{-\\frac{1}{2}}\\text{exp}\\left[-\\frac{(\\mu_i-\\mu)^2}{2\\sigma_i^2}\\right]\\\\\n",
    "&=(2\\pi\\sigma_ 1^ 2\\sigma_ 2^ 2)^{-1}\\text{exp}\\left[-\\frac{1}{2}\\left(\\frac{(\\mu_ 1-\\mu)^2}{\\sigma_ 1^ 2}+ \\frac{(\\mu_ 2-\\mu)^2}{\\sigma_ 2^ 2}\\right)\\right]\n",
    "\\end{aligned}\n",
    "\n",
    "The most likely value of $\\mu$, $\\mu_*$, is the one that maximizes this likelihood. It is more convenient to deal with the log of the likelihood. Maximizing the log likelihood also maximizes the likelihood. \n",
    "\n",
    "$$    \n",
    "    \\text{log }P(D|\\mu,I) = \\text{const} - \\frac{1}{2}\\left[\\frac{(\\mu_ 1-\\mu)^2}        {\\sigma_ 1^ 2} + \\frac{(\\mu_ 2-\\mu)^2}{\\sigma_ 2^ 2}\\right]\n",
    "$$\n",
    "\n",
    "To maximize the log likelihood, we first find its derivative,\n",
    "\n",
    "$$\n",
    "    \\frac{dP(D|\\mu,I)}{d\\mu} = \\frac{\\mu_1-\\mu}{\\sigma_ 1^ 2} + \\frac{\\mu_2-\\mu}         {\\sigma_2^2}\n",
    "$$\n",
    "\n",
    "And then set that derivative equal to zero and solve for $\\mu_*$. \n",
    "\\begin{align}\n",
    "    0 &= \\frac{\\mu_1-\\mu_*}{\\sigma_ 1^ 2} + \\frac{\\mu_2-\\mu_*}{\\sigma_2^2}\\\\\n",
    "    \\mu_* &= \\frac{\\sigma_ 2^ 2\\mu_ 1 + \\sigma_ 1^ 2\\mu_ 2}{\\sigma_ 1^ 2 + \\sigma_ 2 ^ 2}\n",
    "\\end{align}\n",
    "\n",
    "The likelihood for $\\mu$ is Gaussian distributed, and the prior and evidence are both constant with respect to $\\mu$, so the posterior for $\\mu$ is also Guassian distributed. To find the variance of this distribution $\\sigma^ 2$, we can take the second derivative of its logarithm, which is equivalent to the negative inverse of the second derivative of the log likelihood. Also note that the second derivative is negative for all real, non-zero values of $\\sigma_i$, proving that $\\mu_*$ is a maximum. \n",
    "\\begin{aligned}\n",
    "    \\frac{d^2P(D|\\mu,I)}{d\\mu^2} &= -\\frac{1}{\\sigma_ 1^ 2} - \n",
    "        \\frac{1}{\\sigma_ 2^ 2}\\\\\n",
    "     \\sigma^ 2 &= \\frac{\\sigma_ 1^ 2\\sigma_ 2^ 2}{\\sigma_ 1^ 2 + \\sigma_ 2^ 2}\n",
    "\\end{aligned}\n",
    "\n",
    "Therefore, \n",
    "$$\n",
    "    P(\\mu|D,I) \\sim \\mathcal{N}\\left(\\frac{\\sigma_2^2\\mu_1 + \\sigma_1^2\\mu_2} \n",
    "        {\\sigma_ 1^ 2 + \\sigma_ 2 ^ 2}, \n",
    "        \\frac{\\sigma_1^2\\sigma_2^2}{\\sigma_ 1^ 2 + \\sigma_ 2^ 2}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
